{"cells":[{"cell_type":"markdown","source":["Original: https://colab.research.google.com/drive/196b1E_4SievYhi8mxdoS10i72K--ySaK"],"metadata":{"id":"1_qVuxgqLBAs"}},{"cell_type":"markdown","metadata":{"id":"_JBERAXX86Ic"},"source":["# CS753 2023 -- Assignment 2\n","\n","This assignment is due on or before **11.59 pm on April 9, 2023**. The submission portal\n","on Moodle will be open until midnight on April 11th with a 5% penalty for each additional day after the 9th. This  assignment adds up to 25 points overall. This is a group assignment. You can form groups of 2-3 students.\n","\n","## **Acknowledgements**\n","\n","* All of Task 0's ASR-related code snippets have been borrowed from the ASR Notebook at [CS224S's Assignment 4 at Stanford](https://web.stanford.edu/class/cs224s/assignments/a4/) which are, in turn, borrowed from the [SpeechBrain toolkit](https://github.com/speechbrain/speechbrain/). \n","\n","* SpeechBrain models are downloaded from their host site on [Huggingface](https://huggingface.co/speechbrain).  \n","\n","* The following [SpeechBrain tutorial](https://colab.research.google.com/drive/1aFgzrUv3udM_gNJNUoLaHIm78QHtxdIz?usp=sharing#scrollTo=J6N0Fb51pFnZ) will give you a code walkthrough of how an ASR system is coded from scratch using this toolkit. \n","\n","## **Dataset** \n","\n","We will use *HarperValleyBank (HVB)* -- a publicly-available spoken dialog corpus. Click [here](https://arxiv.org/pdf/2010.13929.pdf) for more details about the HVB corpus, how it was collected and what it is annotated for.  \n","\n","## **What to submit**\n","\n","On Moodle, you will have to submit a text file `README.txt` with answers requested for in the tasks below and a `.ipynb` file containing all your code with the following naming convention: LDAP IDs of team members delimited by `_`. E.g., `220022022_220022021.ipynb`.\n","\n","## **Getting Started**\n","* Make a copy of this notebook in your personal Google Drive to make edits. \n","* Change your runtime type (under \"Runtime\") to GPU. \n","* Go through all the steps in Task 0 to get set up with the first ASR task. \n","* **Important:** ASR training, as in Task 0, will take close to **1.5 hours** for two epochs. Keep this in mind when scheduling your runs. You should consider saving the checkpoints from a training run if you want to use it for other experiments or for additional finetuning. \n"]},{"cell_type":"markdown","metadata":{"id":"b5qc5LC2cWtS"},"source":["# Dependencies "]},{"cell_type":"markdown","metadata":{"id":"oTYjUyWirYR3"},"source":["If you have any issues using `gdown`, the same data and config files are available directly via Google Drive [here](https://drive.google.com/drive/folders/1xQxvR9NRlwK-75KMd0i1y4Dy0fimTsM9).  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"etpTb5AW8uM0"},"outputs":[],"source":["# setup\n","# !gdown 1oJh0U3g_bUx6UPX4xix2UHMVHeCE_H1y\n","!gdown 1_OXiLOL2RBsbdCb4WyQsLudYxzJxMDJr\n","!unzip -q hvb.zip\n","!mv content/data /content/\n","!rm -r /content/content\n","\n","!gdown 1a0EGlsLbXnGn1xwZoSqT0tcdAQ1L2nfd # train.py\n","!gdown 1yCmjRbxXRxfEN5LXdnE1Zpl8ZOIzdrAO # train.yaml\n","!gdown 1KHmdcLVFI9ontvGmi5J6vfaropGYuKcr # inference.yaml"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"30NIWqcyAD-G"},"outputs":[],"source":["!pip install speechbrain -q\n","\n","import speechbrain as sb\n","from speechbrain.pretrained import EncoderDecoderASR\n","import json\n","import torchaudio\n","import torch\n","from torch import nn\n","from tqdm import tqdm\n","from collections import Counter\n","from IPython.display import Audio\n","from scipy.io import wavfile\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'"]},{"cell_type":"markdown","metadata":{"id":"YwgCA2Oc9h_h"},"source":["# Task 0: Evaluate a pretrained CRDNN model, further finetuned with HVB (5 points)\n","\n","We will first load a [SpeechBrain CRDNN model pretrained on LibriSpeech](https://huggingface.co/speechbrain/asr-crdnn-rnnlm-librispeech). SpeechBrain has some utils with built-in options to source pre-trained models from a repository on HuggingFace. \n","\n","In Task 0, you will first load this pretrained CRDNN model from HuggingFace and use it for inference on the first 500 examples from `test_manifest.json` (included within `hvb.zip` you have already downloaded earlier). Subsequently, you will fine tune this model on the HVB training dataset before reevaluating on the test examples and note the difference in performance. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bqTF_KNI_mGI"},"outputs":[],"source":["crdnn = EncoderDecoderASR.from_hparams(\n","    source='speechbrain/asr-crdnn-rnnlm-librispeech',\n","    savedir='asr-crdnn-rnnlm-librispeech',\n","    run_opts={'device': 'cuda'}\n",")"]},{"cell_type":"markdown","metadata":{"id":"n9Z1gOWCJCCT"},"source":["The manifests we prepared to use with SpeechBrain are jsons with the structure\n","```\n","{\n","    \"15748\": {\n","        \"wav\": \"/content/data/segments/15748.wav\",\n","        \"length\": 1.86,\n","        \"words\": \"WHAT DAY WOULD YOU LIKE FOR YOUR APPOINTMENT\"\n","    },\n","    ...\n","}\n","```\n","\n","We first load them and define a function to batch them into a format that our `EncoderDecoderASR` object can ingest:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NH2kZ0PbAsb1"},"outputs":[],"source":["TEST_SIZE = 200 # for faster processing\n","\n","with open('data/test_manifest.json', 'r') as f:\n","    test_manifest = json.load(f)\n","test_manifest = {\n","    k: v for k, v in list(test_manifest.items())[:TEST_SIZE]\n","}\n","\n","def batchify(manifest, batch_size):\n","    keys = list(manifest.keys())\n","    wav_paths = list(map(lambda x: x['wav'], manifest.values()))\n","    iterable = zip(keys, wav_paths)\n","    num_examples = len(manifest)\n","    for i in range(0, num_examples, batch_size):\n","        batch_wavs = nn.utils.rnn.pad_sequence([\n","            torchaudio.load(path)[0].squeeze(0)\n","            for path in wav_paths[i:min(i + batch_size, num_examples)]\n","        ], batch_first=True)\n","        batch_keys = keys[i:min(i + batch_size, num_examples)]\n","        batch_wav_lens = torch.tensor([\n","            manifest[key]['length'] for key in batch_keys\n","        ])\n","        batch_wav_lens = batch_wav_lens / batch_wav_lens.max()\n","        yield batch_keys, batch_wavs, batch_wav_lens"]},{"cell_type":"markdown","metadata":{"id":"tdRtje1OLiG5"},"source":["Next, we feed our test examples through the pretrained ASR model:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0YCNyfAzFh-y"},"outputs":[],"source":["true_dict = {key: test_manifest[key]['words'] for key in test_manifest}\n","\n","def inference(model, test_manifest, batch_size=8):\n","    torch.cuda.empty_cache()\n","    pred_dict = {}\n","    for keys, wavs, wav_lens in tqdm(batchify(test_manifest, batch_size), total=round(len(test_manifest) / batch_size + 0.5)):\n","        transcriptions, _ = model.transcribe_batch(wavs.to(device), wav_lens.to(device))\n","        for key, transcription in zip(keys, transcriptions):\n","            pred_dict[key] = transcription\n","    return pred_dict\n","\n","pred_dict = inference(crdnn, test_manifest)"]},{"cell_type":"markdown","metadata":{"id":"Yxyq8kjpmhR3"},"source":["## Q1: Evaluate WER of pretrained model predictions (2 points)"]},{"cell_type":"markdown","metadata":{"id":"Dbnw0_ZKL9TQ"},"source":["Check the word error rate on the first 200 test instances in `test_manifest.json`. Note that we want WERs, so we need to split our transcripts into lists of words. \n","\n","You don't need to implement anything new here. Just follow along and ensure you can run the code to obtain WER on the results you just generated. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IFnB0JOfPZoj"},"outputs":[],"source":["# this data structure stores WER information we use later. \n","details_by_utterance = sb.utils.edit_distance.wer_details_by_utterance(\n","    {k: v.split() for k, v in true_dict.items()},\n","    {k: v.split() for k, v in pred_dict.items()},\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JU-xiNDdSabz"},"outputs":[],"source":["# word error rate (WER) summary using data structure we just created\n","sb.utils.edit_distance.wer_summary(details_by_utterance)"]},{"cell_type":"markdown","metadata":{"id":"c0aOgXMKSowv"},"source":["What is the WER value you obtain? Write it down in `README.txt` that you will upload on Moodle, along with your Colab notebook. \n","\n","We expect WER of this pretrained system to be somewhat high on HVB data (around 72%+ WER). That is really quite high! Note that we already re-sampled the audio to 16kHz to make the HVB audio features more similar to the training inputs of the pre-trained model. \n","\n","Often times ASR errors have specific error modes or correlations -- let's see if we can understand where our pretrained system is failing on HVB data. We can start to investigate where our system is making mistakes by checking some of the top missed utterances."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NCOAzZofSoOH"},"outputs":[],"source":["def summarize(detail_dict, true_dict, pred_dict):\n","    print(f\"{detail_dict['key']}: {detail_dict['WER']}\")\n","    print(f\"\\tTrue: {true_dict[detail_dict['key']]}\")\n","    print(f\"\\tPred: {pred_dict[detail_dict['key']]}\")\n","\n","for wer_dict in sb.utils.edit_distance.top_wer_utts(details_by_utterance, 10)[0]:\n","    summarize(wer_dict, true_dict, pred_dict)"]},{"cell_type":"markdown","metadata":{"id":"RrJ-gKFXUHEf"},"source":["Seems that our predictions keep outputting the same word over and over. Let's see why."]},{"cell_type":"markdown","metadata":{"id":"LrHqM-WfzFes"},"source":["Select some examples the model wrongly predicts, and try to build a hypothesis around what in the data is associated with the model making mistakes. Examples of mistake types include:\n","- Repeated wrong word\n","- A few correct words but clearly wrong transcript\n","\n","Give examples of at least 3 audio files and different kinds of errors you have identified in these audio files. Add this to `README.txt`.\n"]},{"cell_type":"markdown","metadata":{"id":"8q3EtR_WWBjA"},"source":["Here's a code snippet to help you get started. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eVNDEHXvWF-D"},"outputs":[],"source":["example = details_by_utterance[1]\n","summarize(example, true_dict, pred_dict)\n","Audio(test_manifest[example['key']]['wav'])"]},{"cell_type":"markdown","metadata":{"id":"A4OjAs6oWnK-"},"source":["(Hint: In our initial checks, poor audio quality seems to be associated with repeated word errors.)"]},{"cell_type":"markdown","metadata":{"id":"iT62DPlesXhK"},"source":["## Q2: Finetune the pretrained CRDNN model (2 points)"]},{"cell_type":"markdown","metadata":{"id":"3Bm4ZQZQmJU_"},"source":["The performance of this pretrained model is disappointing. However, the model was trained on a different data domain than call-center transcripts as in HVB. To see if we can derive better performance on our dataset, we fine-tune the pretrained model with HVB training data and test it. For this experiment, you won't need to modify it much. Just get training working, and you can try adjusting some training or decoding parameters as you like. The key thing to learn here is simply developing an understanding of how things work when finetuning on a new corpus using SpeechBrain. (This is a state-of-the-art approach to building and adjusting ASR models that might be used in industry projects.)\n","\n","We've set up the training script, experiment yaml, and inference yaml for you, but we encourage you to take a look at how it works (and most importantly what a neat ML experiment yaml file looks like). Training this model for 2 epochs on Colab GPUs should take around **1.5 hours**.\n","\n","The model will save checkpoints during training which you can specify for use during inference / testing below. That means you should be able to use a fine-tuned version of the model, even if you don't wait the full time for the model to completely train. It is okay to submit the homework with your fine tuning model partially trained, but not 2 full epochs. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iM2jQrwTmIw2"},"outputs":[],"source":["# this downloads the training and config files for our fine tuning setup\n","!gdown 1v_3Kl8OrUd6_1_D0ZGoYVFEuOKhZ7YMo # train.py\n","!gdown 17cQIpx5kLLMCD23EDaE0EYg2E9LPqMCF # train.yaml\n","!gdown 1CWYOD2PC97gXguW4krc9122HKAraHkYS # inference.yaml"]},{"cell_type":"markdown","metadata":{"id":"qg-TThsjy2Ul"},"source":["**Finetuning with HVB data**: \n","\n","There are two files you've just downloaded which specify the architecture to train, and the main training loop for improving the neural net ASR system. \n","\n","- `train.yaml` is the yaml config file SpeechBrain uses to specify both the network / ASR system architecture, as well as the parameters of training procedures, loss functions, datasets, etc. This is a good starting point for understanding the architecture of the ASR system you're working with. Note that you are not able to modify much about the ASR network architecture as it needs to match what we load from file. You can adjust things like loss function weights, learning rate, and training time to adjust the fine tuning setup. \n","- `train.py` specifies the main training loop for fitting the acoustic model. You do not need to modify this file. \n","\n","Edit the training yaml file and run the training loop as shown below. Finetune with modified hyperparameters that worked best for you. Copy/paste train loss, valid loss, valid CER and valid WER from your training output in `README.txt`, along with the epoch number. (With using `train.yaml` as-is, we obtained training and validation loss < 1.75 after epoch 1.) \n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HpCTGMTGohrq"},"outputs":[],"source":["torch.cuda.empty_cache()\n","\n","!python train.py train.yaml --batch_size=4\n","# OOM on batch_size=5"]},{"cell_type":"markdown","source":["## Q3: Evaluate your finetuned model (1 point)"],"metadata":{"id":"QLXP3qH_AxCN"}},{"cell_type":"markdown","metadata":{"id":"bOM1BF3TtRB0"},"source":["To run inference, you need to use a different yaml to be compatible with the `EncoderDecoderASR` class. \n","\n","NOTE: You need to set your checkpoint path in a few locations to make this work. Be careful your paths are set before other debugging if things aren't working (e.g. trying to download from HuggingFace)\n","\n","To get inference working there are three steps:\n","1. Note the directory that your checkpoints are saved in (under `./results/CRDNN_BPE_960h_LM/2602/save/{your ckpt here}`)\n","2. Paste this directory into the ckptdir entry in `inference.yaml`\n","3. Paste this directory after the `ckpt_path = ` in the below cell.\n","\n","You can then use the same inference procedure as in Q1.\n","NOTE: set the `ckpt_path` below AND change the path in `inference.yaml` (by modifying `ckptdir` to point to the desired checkpoint) before or after it is copied into your checkpoint path.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S3ScTzkkqMYY"},"outputs":[],"source":["ckpt_path = \"/content/results/CRDNN_BPE_960h_LM/2602/save/CKPT+2023-04-08+06-09-55+00\"\n","!cp inference.yaml {ckpt_path}"]},{"cell_type":"markdown","metadata":{"id":"09717HkzuXWR"},"source":["Evaluate your finetuned model on the first 50 test sentences in `test_manifest.json`. Generate predictions by setting up a model object, and calling `inference()`. Remember your checkpoint paths must be set correctly in the copy of inference.yaml read to run inference. For this step simply populate pred_dict with inferences from your test subset. Write down the WER in `README.txt`. Also compute the WER from the pretrained model for this test subset of size 50 and write it down in `README.txt`.\n","\n","NOTE: Running inference on ~50 utterances might require ~15 minutes of computation on a Colab CPU. The code below uses CPU inference as we could not get checkpoint-loaded DNNs to work with SpeechBrain's inference on the GPU (you are free to try this). "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ylj89CJOsm91"},"outputs":[],"source":["device = 'cpu'\n","our_model = EncoderDecoderASR.from_hparams(\n","    source=ckpt_path, \n","    hparams_file='inference.yaml', \n","    savedir=\"our_ckpt\",\n","    run_opts={'device': device}\n",")"]},{"cell_type":"code","source":["##############################\n","#### YOUR CODE GOES BELOW #####\n","##############################\n","\n","### Populate test_manifest with the first 50 test sentences and then call inference() below###\n","EVAL_SIZE = 50\n","test_manifest = {\n","    k: v for k, v in list(test_manifest.items())[:EVAL_SIZE]\n","}\n","true_dict = {key: test_manifest[key]['words'] for key in test_manifest}\n","\n","pred_dict = inference(our_model.to(device), test_manifest)\n","# this data structure stores WER information we use later. \n","details_by_utterance = sb.utils.edit_distance.wer_details_by_utterance(\n","    {k: v.split() for k, v in true_dict.items()},\n","    {k: v.split() for k, v in pred_dict.items()},\n",")\n","# word error rate (WER) summary using data structure we just created\n","sb.utils.edit_distance.wer_summary(details_by_utterance)"],"metadata":{"id":"9j2daItrFBMA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pred_dict = inference(crdnn, test_manifest)\n","# this data structure stores WER information we use later. \n","details_by_utterance = sb.utils.edit_distance.wer_details_by_utterance(\n","    {k: v.split() for k, v in true_dict.items()},\n","    {k: v.split() for k, v in pred_dict.items()},\n",")\n","# word error rate (WER) summary using data structure we just created\n","sb.utils.edit_distance.wer_summary(details_by_utterance)"],"metadata":{"id":"N8VyYI2XtAwe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Task 1: Train a sentiment detection model (20 points)\n"],"metadata":{"id":"p5PcGftQGwGo"}},{"cell_type":"markdown","source":["Apart from the audio files and transcriptions, the HVB corpus also comes with annotations for intent, sentiment/emotion and dialog actions. Use the commands below to download `transcript.zip` and `hvb-audio.zip`. "],"metadata":{"id":"wdkuAs2OG276"}},{"cell_type":"code","source":["# !gdown 1-s2e8dZYSjhVgfo_TL0V_89RZVhGnZ1Y #transcript.zip\n","!gdown 1oCn4PoJO-9XMEh-RtuatRgtKKL6ZyXb6\n","!unzip -q transcript.zip\n","\n","!gdown 1ChdI1XyhmGq9z8Y8M38yXPMob6oPRqPO  #train.txt\n","!gdown 10w15DnUbJcQRBSZWP03qjM6Oq8l7WSVQ  #dev.txt\n","\n","# !gdown 1eimo-BFXZz6Z3FeZK8uC-wT84ji-JVos #hvb-audio.zip\n","!gdown 1xMyXiFpQo3reF5sWFi6MahUQviW4Z1RA\n","!unzip -q hvb-audio.zip\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'"],"metadata":{"id":"cFdcY0sWMYsi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open('transcript/370981f1f0254ebc.json', 'r') as f:\n","    print(f.read())"],"metadata":{"id":"s6gkwNE00zSn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Each transcript json file within `/content/transcript/` refers to a conversation with a list of utterances. Each segment (utterance) is associated with a json object and one of its keys is labeled as \"emotion\" that maps to three probability values associated with positive, negative and neutral sentiments. You can consider the sentiment with maximum probability to be the ground-truth label for each utterance. \n","\n","For this task, you will write new code to create a sentiment classification model for HVB utterances. Here are the various steps:\n","- Create train and dev splits from `train.txt` and `dev.txt` downloaded in the code cell above. To understand the format in `train.txt` (and `dev.txt`), consider a line in  `train.txt`: *010d38f5ada54e0d:1,2,3,4,5,6,7,8*. This refers to conversation-ID 010d38f5ada54e0d with eight utterances appearing sequentially within the file `/content/transcript/010d38f5ada54e0d.json`. These utterances are numbered by the field `index` in the json file. Similarly interpret the lines in `dev.txt`. Use these text files (`train.txt` and `dev.txt`) to create train and dev sets and extract the relevant metadata (\"emotion\") you need from the respective transcript json files. To extract corresponding audio for these segments, the relevant files will be named `010d38f5ada54e0d.wav` within `/content/audio/agent` and `/content/audio/caller` (obtained after you unzip `hvb-audio.zip`). The fields `channel_index`, `index`, `start_ms` and `duration_ms` within `/content/transcript/010d38f5ada54e0d.json` gives you all the information needed to extract audio for the eight utterances mentioned in the example above. Click [here](https://github.com/cricketclub/gridspace-stanford-harper-valley#field-descriptions) for more details about the json field descriptions. \n","- Load the pretrained CRDNN Librispeech model as in Task 0 and extract the encoder.\n","- Add a linear layer mapping the encoding of the audio signal to a prediction of the underlying sentiment. This will be randomly initialized.\n","- Train both the new linear layer and all the pretrained encoder layers using a cross-entropy loss with the reference emotion labels derived as described at the top of this cell. \n","- Evaluate your trained sentiment detection model on utterances listed in `dev.txt` and compute overall accuracy for sentiment prediction. \n","- Write down the accuracy you obtained in `README.txt`. \n","\n"],"metadata":{"id":"2jjw2l0WMq1f"}},{"cell_type":"markdown","source":["## Rough Score Breakdown\n","\n","---\n","\n","1. Data preprocessing (4 points)\n","2. Loading the CRDNN model and extracting the encoder layers (6 points)\n","3. Adding a linear layer + training the model (7 points)\n","4. Obtaining accuracies similar to or better than our solution code (3 points)  "],"metadata":{"id":"bFy7OG5zjwFJ"}},{"cell_type":"code","source":["!pip install pydub"],"metadata":{"id":"qV-JTx4-o3MY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["##########################################################\n","#### ALL YOUR CODE FOR SENTIMENT DETECTION GOES HERE #####\n","import json\n","from pydub import AudioSegment\n","\n","train_data = []\n","train_audio_agent = []\n","train_audio_caller = []\n","with open('train.txt','r') as f:\n","    for line in f.readlines():\n","        convID = line.split(':')[0]\n","        indices = list(map(int,line.split(':')[1].split(',')))\n","        train_file = []\n","        with open(f'transcript/{convID}.json','r') as f1:\n","            utt = json.load(f1)\n","            for i in indices:\n","                train_obj = {}\n","                train_obj['words'] = utt[i - 1]['transcript']\n","                train_obj['emotion'] = list(utt[i - 1]['emotion'].values())\n","                train_obj['channel_index'] = utt[i - 1]['channel_index']\n","                train_obj['start_ms'] = utt[i - 1]['start_ms']\n","                train_obj['duration_ms'] = utt[i - 1]['duration_ms']\n","                train_obj['length'] = train_obj['duration_ms']/1000\n","                if train_obj['channel_index'] == 1:\n","                    path = f'audio/caller/'\n","                else:\n","                    path = f'audio/agent/'\n","                original_wav = AudioSegment.from_wav(path + str(convID) + '.wav')\n","                extracted_wav = original_wav[train_obj['start_ms']:train_obj['start_ms']+train_obj['duration_ms']]\n","                extracted_wav.export(path + str(convID) + '_' + str(i) + '.wav', format='wav')\n","                train_obj[\"wav\"] = path + str(convID) + '_' + str(i) + '.wav'\n","                train_file.append(train_obj)\n","        train_data.extend(train_file)\n","\n","dev_data = []\n","dev_audio_agent = []\n","dev_audio_caller = []\n","with open('dev.txt','r') as f:\n","    for line in f.readlines():\n","        convID = line.split(':')[0]\n","        indices = list(map(int,line.split(':')[1].split(',')))\n","        train_file = []\n","        with open(f'transcript/{convID}.json','r') as f1:\n","            utt = json.load(f1)\n","            for i in indices:\n","                train_obj = {}\n","                train_obj['words'] = utt[i - 1]['transcript']\n","                train_obj['emotion'] = list(utt[i - 1]['emotion'].values())\n","                train_obj['channel_index'] = utt[i - 1]['channel_index']\n","                train_obj['start_ms'] = utt[i - 1]['start_ms']\n","                train_obj['duration_ms'] = utt[i - 1]['duration_ms']\n","                train_obj['length'] = train_obj['duration_ms']/1000\n","                if train_obj['channel_index'] == 1:\n","                    path = f'audio/caller/'\n","                else:\n","                    path = f'audio/agent/'\n","                original_wav = AudioSegment.from_wav(path + str(convID) + '.wav')\n","                extracted_wav = original_wav[train_obj['start_ms']:train_obj['start_ms']+train_obj['duration_ms']]\n","                extracted_wav.export(path + str(convID) + '_' + str(i) + '_dev.wav', format='wav')\n","                train_obj[\"wav\"] = path + str(convID) + '_' + str(i) + '_dev.wav'\n","                train_file.append(train_obj)\n","        dev_data.extend(train_file)\n","##########################################################"],"metadata":{"id":"xc6tENScx2Tp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["crdnn = EncoderDecoderASR.from_hparams(\n","    source='speechbrain/asr-crdnn-rnnlm-librispeech',\n","    savedir='asr-crdnn-rnnlm-librispeech',\n","    run_opts={'device': 'cuda'}\n",")\n","\n","max_length = 0\n","\n","def getMaxLength(manifest):\n","    global max_length\n","    keys = list(manifest.keys())\n","    wav_paths = list(map(lambda x: x['wav'], manifest.values()))\n","    iterable = zip(keys, wav_paths)\n","    num_examples = len(manifest)\n","    for i in range(0, num_examples):\n","        batch_wavs = nn.utils.rnn.pad_sequence([\n","            torchaudio.load(path)[0].squeeze(0)\n","            for path in wav_paths[i:min(i + 1, num_examples)]\n","        ], batch_first=True)\n","        batch_keys = keys[i:min(i + 1, num_examples)]\n","        batch_wav_lens = torch.tensor([\n","            manifest[key]['length'] for key in batch_keys\n","        ])\n","        batch_wav_lens = batch_wav_lens / batch_wav_lens.max()\n","        max_length = max(max_length, batch_wavs.shape[1])\n","\n","train_manifest = {key: train_data[key] for key in range(len(train_data))}\n","\n","getMaxLength(train_manifest)"],"metadata":{"id":"c6k-L4LGIwpW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from torch import nn\n","\n","class Network(nn.Module):\n","\n","    def __init__(self, crdnn):\n","        super().__init__()\n","        self.enc = crdnn.mods.encoder\n","        self.lin = nn.Linear(233*512,3)\n","    \n","    def forward(self, wav, wav_len):\n","        x = self.enc(wav,wav_len).reshape(1,-1)\n","        x = self.lin(x)\n","        return x\n","\n","model = Network(crdnn).to(device)"],"metadata":{"id":"1jBd2Qkl-WSf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# training\n","# https://speechbrain.readthedocs.io/en/latest/API/speechbrain.pretrained.interfaces.html#speechbrain.pretrained.interfaces.EncoderDecoderASR\n","from tqdm.notebook import tqdm\n","\n","def mybatchify(manifest, batch_size):\n","    global max_length\n","    keys = list(manifest.keys())\n","    wav_paths = list(map(lambda x: x['wav'], manifest.values()))\n","    iterable = zip(keys, wav_paths)\n","    num_examples = len(manifest)\n","    for i in range(0, num_examples, batch_size):\n","        batch_wavs = nn.utils.rnn.pad_sequence([\n","            torchaudio.load(path)[0].squeeze(0)\n","            for path in wav_paths[i:min(i + batch_size, num_examples)]\n","        ], batch_first=True)\n","        batch_wavs = torch.cat([batch_wavs, torch.zeros(batch_wavs.size(0), max_length - batch_wavs.size(1))], dim=1)\n","        batch_keys = keys[i:min(i + batch_size, num_examples)]\n","        batch_wav_lens = torch.tensor([\n","            manifest[key]['length'] for key in batch_keys\n","        ])\n","        batch_wav_lens = batch_wav_lens / batch_wav_lens.max()\n","        emotions = torch.tensor([\n","            manifest[key]['emotion'] for key in batch_keys\n","        ])\n","        yield batch_keys, batch_wavs, batch_wav_lens, emotions\n","\n","def train(model, train_manifest, batch_size=8):\n","    torch.cuda.empty_cache()\n","    model.train()\n","    optim = torch.optim.Adam(model.parameters(),lr=0.0001,weight_decay=0.01)\n","    loss_fn = torch.nn.functional.cross_entropy\n","    total_loss = 0\n","    i = 0\n","    pred_dict = {}\n","    for keys, wavs, wav_lens, emotions in tqdm(mybatchify(train_manifest, batch_size), total=round(len(train_manifest) / batch_size + 0.5)):\n","        optim.zero_grad()\n","        preds = model(wavs.to(device), wav_lens.to(device))\n","        emotions = emotions.to(device)\n","        max_emotion = emotions // emotions.max()\n","        loss = torch.nn.functional.cross_entropy(preds,max_emotion)\n","        # if i % 20 == 0:\n","        #     print(loss.item())\n","        i += 1\n","        total_loss += loss.item()\n","        loss.backward()\n","        optim.step()\n","\n","train(model,train_manifest,1)"],"metadata":{"id":"GlQ60okRw2sr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Populate test_manifest with the first 50 test sentences and then call inference() below###\n","def evaluate(model, test_manifest, batch_size=8):\n","    torch.cuda.empty_cache()\n","    model.eval()\n","    pred_dict = {}\n","    score = 0\n","    for keys, wavs, wav_lens, emotions in tqdm(mybatchify(test_manifest, batch_size), total=round(len(test_manifest) / batch_size + 0.5)):\n","        preds = model(wavs.to(device), wav_lens.to(device))\n","        emotions = emotions.to(device)\n","        for i in range(preds.shape[0]):\n","            if torch.argmax(preds[i]) == torch.argmax(emotions[i]):\n","                score += 1\n","    print(score / len(test_manifest))\n","\n","test_manifest = {key: dev_data[key] for key in range(len(dev_data))}\n","\n","evaluate(model, test_manifest,1)"],"metadata":{"id":"lAjpfugWtQ8u"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Extra Credit: Use Whisper's pretrained model to evaluate HVB (5 points)"],"metadata":{"id":"qVTg1u4xM-Ez"}},{"cell_type":"markdown","source":["Write code to evaluate [Whisper's **small** model](https://github.com/openai/whisper/blob/main/model-card.md) on the first 200 test utterances in `test_manifest.json` that you used in Q1 of Task 0. Compute the WER with predictions from Whisper and add it to `README.txt`."],"metadata":{"id":"HQbIF2iyNE5H"}},{"cell_type":"code","source":["!pip install openai-whisper"],"metadata":{"id":"ibeWcRNRCXwW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["##############################################################\n","#### YOUR EVALUATION CODE USING Whisper-small GOES BELOW #####\n","import whisper\n","\n","TEST_SIZE = 200\n","\n","with open('data/test_manifest.json', 'r') as f:\n","    test_manifest = json.load(f)\n","test_manifest = {\n","    k: v for k, v in list(test_manifest.items())[:TEST_SIZE]\n","}\n","true_dict = {key: test_manifest[key]['words'] for key in test_manifest}\n","\n","whisp = whisper.load_model('small').to('cuda')\n","\n","pred_dict = {}\n","for key in test_manifest:\n","    audio = test_manifest[key]['wav']\n","    result = whisp.transcribe(audio)\n","    pred_dict[key] = result['text'].upper()\n","\n","# this data structure stores WER information we use later. \n","details_by_utterance = sb.utils.edit_distance.wer_details_by_utterance(\n","    {k: v.split() for k, v in true_dict.items()},\n","    {k: v.split() for k, v in pred_dict.items()},\n",")\n","# word error rate (WER) summary using data structure we just created\n","sb.utils.edit_distance.wer_summary(details_by_utterance)\n","##############################################################"],"metadata":{"id":"hIC3SBBzxtlS"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"196b1E_4SievYhi8mxdoS10i72K--ySaK","timestamp":1680794530744}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}